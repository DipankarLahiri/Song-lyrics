---
title: "Comparing lyrics of Coldplay, Eminem, Drake"
output:   
  html_document:
    df_print: paged
authors: Dipankar Lahiri
updated: "12/11/2024"
---

This is a word frequency analysis using parts-of-speech tagging of the entire lyrics of three popular music artists - Coldplay, Eminem and Drake. The differences in the lyrics highlight the contrasting approaches to songwriting in alternative rock and rap/hip-hop.

## Highlights of Findings

**1. Favourite Words and Themes**

Coldplay’s most common words are about emotions and abstract themes like 'love', 'world', 'time', 'life'. Their verbs ('feel', 'hear', 'wait', 'live', 'cry') suggest introspection and emotional labour. Reflections on emotions, the temporality of experience and cosmic elements are some broad themes that can be identified from Coldplay's most common words.

Drake and Eminem lean heavily on slang and explicit language. Their top words include nigga, shit, bitch, fuck, money, and baby — terms tied to rap’s linguistic identity. Drake's favourite words are 'nigga', 'shit', 'girl', 'time', 'love', 'fuck', 'money', 'baby', 'life' and 'bitch'. Eminem's favourite words are 'shit', 'fuck', 'bitch', 'time', 'shady', 'ass', 'day', leave', 'feel' and 'girl'.

**2. Adjective Use**

Adjective use is very distinct for all three artists. 'Wrong' is the most common adjective.

Coldplay has a completely distinct set of adjectives from the other two, 'close' being the most popular word. 'True', 'cold', 'dark', 'beautiful', 'free' are some other popular descriptions.

'Real', 'bad' and 'crazy' are some common top adjectives between Drake and Eminem, though 'real' occurs many more times in Drake's lyrics and 'bad' occurs many more times in Eminem's. 

'Hard' is a top adjective for Drake but not Eminem. 'White' is a top adjective for Eminem but not Drake.

**3. Verb Use**

Top verbs are more common across all artists. 'Feel', 'hear' and 'start' are three high-occuring verbs common between all three artists. 

Some unique common verbs for Coldplay are 'wait', 'stand', 'cry' and 'fly'. Use of the words 'leave', 'stay' and 'hit' is common for Drake and Eminem and not for Coldplay. 

'Play' is a verb commonly used by Drake only, 'fuck' and 'kill' are verbs commonly used by Eminem only. 

**4. Adverb Use**

Coldplay's most common adverb is 'forever'. 'Damn' is the most common adverb used by Drake and Eminem.

-------------------------------------------------------------------------------------------------------------

```{r setup}

knitr::opts_chunk$set(dev = 'png')

library(udpipe) 
library(dplyr)
library(tidyverse)
library(tm)
library(ggplot2)
library(tidytext)
library(stringr)
library(forcats)
library(wordcloud)
library(RColorBrewer)
library(grid)
library(gridExtra)

my_df <- read_csv("data_challengeA+B/ColdPlay.csv")
my_df <- my_df %>% select(-1)
```

## Data Cleaning, Preparation

Loading libraries, cleaning dataset of Coldplay's lyrics to make it amenable to analysis (making Coldplay's 60 albums levels, arranging songs by release date). Deduplicating songs (to avoid over-representation of words from multiple versions of the same song). Tokenising lyrics into words, removing stopwords.

```{r}
my_df <- my_df %>%
  mutate(Album = factor(Album, exclude = NULL),
         Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  arrange(Date)

my_df <- my_df %>%
  mutate(Title_clean = tolower(Title) %>%
                     str_replace_all("\\(.*\\)|\\[.*\\]", "") %>% 
                     str_replace_all("[^a-z0-9 ]", "") %>% 
                     str_trim()) %>%
  distinct(Title_clean, .keep_all = TRUE) %>%
  select(-Title_clean)
```

Dataframe my_df now has 245 songs as rows, down from 344 in the csv file, which means 99 songs in the dataset were additional versions of same songs. It has columns for Title, Album, Year and Date. These can be used for future analysis, visualisations.

```{r}
my_df <- my_df %>%
  mutate(Lyric = str_replace_all(Lyric, "[^\\w\\s'’-]", " ") %>%
                   stringi::stri_trans_general("NFC"))

my_df_tokens <- my_df %>%
  unnest_tokens(word, Lyric, token = "regex", pattern = "([^\\w'-]+)") %>%
  filter(!str_detect(word, "^\\d+$")) 

data("stop_words")
stop_words_clean <- stop_words %>% filter(!str_detect(word, "'"))

my_df_tokens <- my_df_tokens %>%
  anti_join(stop_words_clean, by = "word") 
```

Created new dataframe my_df_tokens. Here, all words from the Lyric column of my_df are split up and given separate rows, preserving contractions, possessives, and hyphenated words. Then, eliminated all stop word and numbers. This first creates a dataset of 42,267 rows, before filtering, resulting in 15,813 rows for all meaningful words. 

Loading parts-of-speech model, tagging words in my_df_tokens with this model

```{r}
ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model("/Users/dipankarlahiri/Desktop/College/Sem 2/Data Analysis and Collection/R/Projects/Training/Song lyrics/SongLyricChallenges/english-ewt-ud-2.5-191206.udpipe")

my_df_pos <- udpipe_annotate(ud_model, x = my_df_tokens$word) %>%
  as.data.frame()

my_df_pos <- my_df_pos %>%
  mutate(token = case_when(
    tolower(lag(token)) %in% c("do", "does", "did", "would", "should", "could", "ca") & tolower(token) == "n't" ~ paste0(lag(token), "n't"),
    tolower(lag(token)) %in% c("i", "you", "he", "she", "it", "we", "they") & token %in% c("'m", "'ve", "'ll", "'d", "'re") ~ paste0(lag(token), token),  
    tolower(lag(token)) %in% c("that", "who", "where", "how", "what", "there") & token == "'s" ~ paste0(lag(token), "'s"),
    tolower(lag(token)) == "o" & token == "'er" ~ "o'er",
    tolower(lag(token)) == "rock" & token == "'n" & lead(token) == "'roll" ~ "rock’n’roll",
    tolower(lag(token)) == "let" & token == "'s" ~ "let's",
    tolower(lag(token)) == "ma" & token == "'am" ~ "ma'am",
    tolower(lag(token)) == "y" & token == "'all" ~ "y'all",
    tolower(lag(token)) == "c" & token == "'mon" ~ "c'mon",
    tolower(lag(token)) == "would" & tolower(token) == "nt" ~ "wouldn't",
    tolower(lag(token)) == "could" & token == "'ve" ~ "could've",
    tolower(lag(token)) == "should" & token == "'ve" ~ "should've",
    lag(token) == "ca" & token == "n't" ~ "can't",  # Fix for "ca"
    TRUE ~ token
  )) %>%
  filter(!token %in% c("n't", "'m", "'ve", "'ll", "'d", "'s", "'n", "'roll", "'am", "'you", "nt", "'all", "'mon", "'", "ca"))

my_df_pos <- my_df_pos %>%
  filter(!(lag(token) == token & row_number() > 1))
```

Created new dataframe my_df_pos by tagging words from my_df_tokens using a parts-of-speech model. This initially increased the row count to 18,000 due to the model splitting contractions, possessives, and compound words into separate tokens. To address this, merged key contractions and compound words back into single tokens while preserving their grammatical structure. After this adjustment, my_df_pos now has 16,127 rows.

## Coldplay's Top Words 

**Verbs -** feel, hear, wait, be, live, fly, talk, start, cry, stand

```{r}
pos_counts <- my_df_pos %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE)

top_verbs <- pos_counts %>% filter(upos == "VERB") %>% slice_head(n = 15)
list(Verbs = top_verbs)
```

**Nouns -** love, world, time, day, life, light, sky, head, heart, sun

```{r}
top_nouns <- pos_counts %>% filter(upos == "NOUN") %>% slice_head(n = 15)
list (Nouns = top_nouns)
```

**Adjectives -** close, wrong, true, alright, careful, dark, cold, strong, beautiful, free

```{r}
top_adjectives <- pos_counts %>% filter(upos == "ADJ") %>% slice_head(n = 15)
list(Adjectives = top_adjectives)
```

**Adverbs -** forever, inside, here, all, slowly, underneath, pretty, where

```{r}
top_adverbs <- pos_counts %>% filter(upos == "ADV") %>% slice_head(n = 15)
list(Adverbs = top_adverbs)
```

**Top Overall Words -** love (218), world (127), time (112), day (107), feel (101)

### Top Themes 

- 'Love' is the most common word. 'Feel' is the only non-noun word to occur more than 100 times. 'Life' and 'Heart' are other top words that can be said to fall under a broad theme of emotional reflection.

- 'Time' and 'Day', two of the other top 5 words overall, these can be seen to represent a theme of temporality. 'Forever', the most used adverb, 'Wait' and 'Start' fall under this theme.

- 'World', the second most common word overall, represents a theme of the cosmos. This includes natural elements like 'Light', 'Sky' and 'Sun'.

**Limitations** 

Counting word frequencies can often miss the context in which the word is used. Words being misclassified (Eg: lose as adjective) suggest limitations of the POS tagging model and the appearance of words like 'Se' suggest limitations in the lemmatisation.

-------------------------------------------------------------------------------------------------------------

## Drake's Top Words

```{r}

my_df_drake <- read_csv("data_challengeA+B/Drake.csv")
my_df_drake <- my_df_drake %>% select(-1)
my_df_drake <- my_df_drake %>%
  mutate(Album = factor(Album, exclude = NULL),
         Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  arrange(Date)

my_df_drake <- my_df_drake %>%
  mutate(Title_clean = tolower(Title) %>%
                     str_replace_all("\\(.*\\)|\\[.*\\]", "") %>% str_replace_all("[^a-z0-9 ]", "") %>% 
                     str_trim()) %>%
  distinct(Title_clean, .keep_all = TRUE) %>%
  select(-Title_clean)

my_df_drake <- my_df_drake %>%
  mutate(Lyric = str_replace_all(Lyric, "[^\\w\\s'’-]", " ") %>%
                   stringi::stri_trans_general("NFC"))

my_df_drake_tokens <- my_df_drake %>%
  unnest_tokens(word, Lyric, token = "regex", pattern = "([^\\w'-]+)") %>%
  filter(!str_detect(word, "^\\d+$")) 

my_df_drake_tokens <- my_df_drake_tokens %>%
  anti_join(stop_words_clean, by = "word") 

my_df_drake_pos <- udpipe_annotate(ud_model, x = my_df_drake_tokens$word) %>%
  as.data.frame()

my_df_drake_pos <- my_df_drake_pos %>%
  mutate(token = case_when(
    tolower(lag(token)) %in% c("do", "does", "did", "would", "should", "could", "ca") & tolower(token) == "n't" ~ paste0(lag(token), "n't"),
    tolower(lag(token)) %in% c("i", "you", "he", "she", "it", "we", "they") & token %in% c("'m", "'ve", "'ll", "'d", "'re") ~ paste0(lag(token), token),  
    tolower(lag(token)) %in% c("that", "who", "where", "how", "what", "there") & token == "'s" ~ paste0(lag(token), "'s"),
    tolower(lag(token)) == "o" & token == "'er" ~ "o'er",
    tolower(lag(token)) == "rock" & token == "'n" & lead(token) == "'roll" ~ "rock’n’roll",
    tolower(lag(token)) == "let" & token == "'s" ~ "let's",
    tolower(lag(token)) == "ma" & token == "'am" ~ "ma'am",
    tolower(lag(token)) == "y" & token == "'all" ~ "y'all",
    tolower(lag(token)) == "c" & token == "'mon" ~ "c'mon",
    tolower(lag(token)) == "would" & tolower(token) == "nt" ~ "wouldn't",
    tolower(lag(token)) == "could" & token == "'ve" ~ "could've",
    tolower(lag(token)) == "should" & token == "'ve" ~ "should've",
    lag(token) == "ca" & token == "n't" ~ "can't",  
    TRUE ~ token
  )) %>%
  filter(!token %in% c("n't", "'m", "'ve", "'ll", "'d", "'s", "'n", "'roll", "'am", "'you", "nt", "'all", "'mon", "'", "ca"))

my_df_drake_pos <- my_df_drake_pos %>%
  filter(!(lag(token) == token & row_number() > 1))

pos_counts_drake <- my_df_drake_pos %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE)

top_verbs_drake <- pos_counts_drake %>% filter(upos == "VERB") %>% slice_head(n = 15)
list(Verbs = top_verbs_drake)

top_nouns_drake <- pos_counts_drake %>% filter(upos == "NOUN") %>% slice_head(n = 15)
list (Nouns = top_nouns_drake)

top_adjectives_drake <- pos_counts_drake %>% filter(upos == "ADJ") %>% slice_head(n = 15)
list (Adjectives = top_adjectives_drake)

top_adverbs_drake <- pos_counts_drake %>% filter(upos == "ADV") %>% slice_head(n = 15)
list(Adverbs = top_adverbs_drake)

```

**Drake's Top Words -** Nigga, Shit, Girl, Time, Love, Fuck, Money, Baby, Life, Bitch (all nouns)

**Drake's Top Non-Noun Words -** Real (ADJECTIVE - 268), Feel (VERB - 259), Start (VERB - 240), Leave (VERB - 233), Hear (Verb - 200)

## Eminem

```{r}

my_df_eminem <- read_csv("data_challengeA+B/Eminem.csv")
my_df_eminem <- my_df_eminem %>% select(-1)
my_df_eminem <- my_df_eminem %>%
  mutate(Album = factor(Album, exclude = NULL),
         Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  arrange(Date)

my_df_eminem <- my_df_eminem %>%
  mutate(Title_clean = tolower(Title) %>%
                     str_replace_all("\\(.*\\)|\\[.*\\]", "") %>% str_replace_all("[^a-z0-9 ]", "") %>% 
                     str_trim()) %>%
  distinct(Title_clean, .keep_all = TRUE) %>%
  select(-Title_clean)

my_df_eminem <- my_df_eminem %>%
  mutate(Lyric = str_replace_all(Lyric, "[^\\w\\s'’-]", " ") %>%
                   stringi::stri_trans_general("NFC"))

my_df_eminem_tokens <- my_df_eminem %>%
  unnest_tokens(word, Lyric, token = "regex", pattern = "([^\\w'-]+)") %>%
  filter(!str_detect(word, "^\\d+$")) 

my_df_eminem_tokens <- my_df_eminem_tokens %>%
  anti_join(stop_words_clean, by = "word") 

my_df_eminem_pos <- udpipe_annotate(ud_model, x = my_df_eminem_tokens$word) %>%
  as.data.frame()

my_df_eminem_pos <- my_df_eminem_pos %>%
  mutate(token = case_when(
    tolower(lag(token)) %in% c("do", "does", "did", "would", "should", "could", "ca") & tolower(token) == "n't" ~ paste0(lag(token), "n't"),
    tolower(lag(token)) %in% c("i", "you", "he", "she", "it", "we", "they") & token %in% c("'m", "'ve", "'ll", "'d", "'re") ~ paste0(lag(token), token),  
    tolower(lag(token)) %in% c("that", "who", "where", "how", "what", "there") & token == "'s" ~ paste0(lag(token), "'s"),
    tolower(lag(token)) == "o" & token == "'er" ~ "o'er",
    tolower(lag(token)) == "rock" & token == "'n" & lead(token) == "'roll" ~ "rock’n’roll",
    tolower(lag(token)) == "let" & token == "'s" ~ "let's",
    tolower(lag(token)) == "ma" & token == "'am" ~ "ma'am",
    tolower(lag(token)) == "y" & token == "'all" ~ "y'all",
    tolower(lag(token)) == "c" & token == "'mon" ~ "c'mon",
    tolower(lag(token)) == "would" & tolower(token) == "nt" ~ "wouldn't",
    tolower(lag(token)) == "could" & token == "'ve" ~ "could've",
    tolower(lag(token)) == "should" & token == "'ve" ~ "should've",
    lag(token) == "ca" & token == "n't" ~ "can't",  
    TRUE ~ token
  )) %>%
  filter(!token %in% c("n't", "'m", "'ve", "'ll", "'d", "'s", "'n", "'roll", "'am", "'you", "nt", "'all", "'mon", "'", "ca"))

my_df_eminem_pos <- my_df_eminem_pos %>%
  filter(!(lag(token) == token & row_number() > 1))

pos_counts_eminem <- my_df_eminem_pos %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE)

top_verbs_eminem <- pos_counts_eminem %>% filter(upos == "VERB") %>% slice_head(n = 15)
list(Verbs = top_verbs_eminem)

top_nouns_eminem <- pos_counts_eminem %>% filter(upos == "NOUN") %>% slice_head(n = 15)
list (Nouns = top_nouns_eminem)

top_adjectives_eminem <- pos_counts_eminem %>% filter(upos == "ADJ") %>% slice_head(n = 15)
list (Adjectives = top_adjectives_eminem)

top_adverbs_eminem <- pos_counts_eminem %>% filter(upos == "ADV") %>% slice_head(n = 15)
list(Adverbs = top_adverbs_eminem)

```

**Eminem's Top Words -** Shit, Fuck, Bitch, Time, Shady, Ass, Day, Leave (Verb), Feel (Verb), Girl


```{r}

set.seed(42)

my_df_drake_pos_sample <- my_df_drake_pos %>% sample_n(16127)
my_df_eminem_pos_sample <- my_df_eminem_pos %>% sample_n(16127)

pos_counts_coldplay <- my_df_pos %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE) %>%
  mutate(Artist = "Coldplay")

pos_counts_drake <- my_df_drake_pos_sample %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE) %>%
  mutate(Artist = "Drake")

pos_counts_eminem <- my_df_eminem_pos_sample %>%
  filter(upos %in% c("VERB", "NOUN", "ADJ", "ADV")) %>%
  count(upos, lemma, sort = TRUE) %>%
  mutate(Artist = "Eminem")

top_words_all <- bind_rows(pos_counts_coldplay, pos_counts_drake, pos_counts_eminem)
top_words_all <- top_words_all %>%
  filter(upos %in% c("NOUN", "VERB", "ADJ", "ADV")) %>%
  group_by(Artist, upos) %>%
  slice_max(n = 10, order_by = n) %>% 
  ungroup()

ggplot(top_words_all, aes(x = fct_reorder(lemma, n), y = n, fill = Artist)) +
  geom_col(position = "dodge") +
  facet_wrap(~ upos, scales = "free") +
  coord_flip() +
  labs(title = "Top 10 Words in Lyrics by POS Category",
       x = "Words",
       y = "Frequency") +
  theme_minimal()

```

```{r}

plot_wordcloud <- function(artist_name, color_palette) {
  data_subset <- subset(top_words_all, Artist == artist_name)
  wordcloud(words = data_subset$lemma, freq = data_subset$n, 
            scale = c(2, 0.4),
            min.freq = 2,
            max.words = 120,
            random.order = FALSE, 
            colors = brewer.pal(8, color_palette))
}

plot_wordcloud("Coldplay", "Blues")
plot_wordcloud("Drake", "Reds")
plot_wordcloud("Eminem", "Purples")

```
